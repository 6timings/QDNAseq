%\VignetteIndexEntry{Introduction to QDNAseq}

\documentclass{article}

\begin{document}

\title{Introduction to QDNAseq}
\author{Ilari Scheinin}
\maketitle

\section{Running QDNAseq}

This is a short tutorial on how to use the QDNseq package. It covers an example
run using the included data set of chromosome 9 of a low grade glioma (LGG) s
ample. First step is naturally to load the package.

<<>>=
library(QDNAseq)
@

Then we need to obtain bin annotations. These are available pre-calculated for
genome build hg19 and bin sizes 1, 5, 10, 15, 30, 50, 100, 500, and 1000 kbp.
They can be downloaded for example with:

<<eval=FALSE>>=
bins <- downloadBinAnnotations(binSize=15)
@

If you are working with another genome build (or another species), see the
section on generating the bin annotations.

Next step is to load the sequencing data from bam files. This can be done for example with one of the commands below.

<<eval=FALSE>>=
readCounts <- binReadCounts(bins)
# all files ending in .bam from the current working directory

# or

readCounts <- binReadCounts(bins, bamfiles='tumor.bam')
# file 'tumor.bam' from the current working directory

# or

readCounts <- binReadCounts(bins, path='tumors')
# all files ending in .bam from the subdirectory 'tumors'
@

This will return an object of class QDNAseqReadCounts. For the purpose of this
tutorial, we load the example data set of chromosome 9 of low grade glioma
sample LGG150.

<<>>=
data(LGG150)
readCounts <- LGG150
readCounts
@

Plot a raw copy number profile (read counts across the genome), and highlight bins removed with default filtering (Figure~\ref{fig:rawprofile}).

<<label=rawprofile, eval=FALSE>>=
plot(readCounts, logTransform=FALSE)
highlightFilters(readCounts, logTransform=FALSE, residual=TRUE, blacklist=TRUE)
@

\begin{figure}[h]
	\centering
<<fig=TRUE, echo=FALSE>>=
<<rawprofile>>
@
	\caption{Read counts per bins. Highlighted with red are bins that will be
		filtered out.}
	\label{fig:rawprofile}
\end{figure}

Apply filters and plot median read counts as a function of GC content and
mappability (Figure~\ref{fig:isobar}). As the example data set only contains
chromosome 9 and not the whole genome, this looks jagged. Normally, a smoother
distribution is to be expected.

<<label=isobar, eval=FALSE>>=
readCountsFiltered <- applyFilters(readCounts, residual=TRUE, blacklist=TRUE)
isobarPlot(readCountsFiltered)
@

\begin{figure}[h]
	\centering
<<fig=TRUE, echo=FALSE>>=
<<isobar>>
@
	\caption{Median read counts per bin shown as a function of GC content and
		mappability. As the example data only contains chromosome 9, this looks
		a lot less smooth than a whole-genome plot should.}
	\label{fig:isobar}
\end{figure}

Estimate the correction for GC content and mappability, and make a plot for the
relationship between the observed standard deviation in the data and its read
depth (Figure~\ref{fig:noise}). The theoretical expectation is a linear
relationship, which is shown in the plot with a black line. Samples with
low-quality DNA will be noisier than expected and appear further above the line
than good-quality samples.

<<label=noise, eval=FALSE>>=
readCountsFiltered <- estimateCorrection(readCountsFiltered)
noisePlot(readCountsFiltered)
@

\begin{figure}[h]
	\centering
<<fig=TRUE, echo=FALSE>>=
<<noise>>
@
	\caption{The relationship between sequence depth and noise.}
	\label{fig:noise}
\end{figure}

Next, we apply the correction for GC content and mappability. This will return
a QDNAseqCopyNumbers object, which we then normalize, smooth outliers, and plot
the copy number profile.

<<label=profile, eval=FALSE>>=
copyNumbers <- correctBins(readCountsFiltered)
copyNumbersNormalized <- normalizeBins(copyNumbers)
copyNumbersSmooth <- smoothOutlierBins(copyNumbersNormalized)
plot(copyNumbersSmooth)
@

\begin{figure}[h]
	\centering
<<fig=TRUE, echo=FALSE>>=
<<profile>>
@
	\caption{Copy number profile after correcting for GC content and mappability.}
\end{figure}

Data is now ready to be analyzed with a downstream package of choice. Segmentation with the CBS algorithm from DNAcopy and calling copy number aberrations with CGHcall have been implemented for convenience.

<<label=segments, eval=FALSE>>=
copyNumbersSegmented <- segmentBins(copyNumbersSmooth)
plot(copyNumbersSegmented)
@

\begin{figure}[h]
	\centering
<<fig=TRUE, echo=FALSE>>=
<<segments>>
@
	\caption{A plot}
\end{figure}

Tune segmentation parameters and iterate until satisfied. Next, call aberrations, and plot the final results.

<<eval=FALSE>>=
copyNumbersCalled <- callBins(copyNumbersSegmented)
plot(copyNumbersCalled)
@

Finally, for other downstream analyses, such as running CGHregions, it might be useful to convert to a cghCall object.

<<eval=FALSE>>=
cgh <- makeCgh(copyNumbersCalled)
@

This command can also be used to generate cghRaw or cghSeg objects by running
it before segmentation or calling.

% caching

% 661 bam files = 445G
% cache/reads = 13G
% cache/readCounts/15kbp = 139M
% cache/readCounts/30kbp = 82M
% cache/readCounts/100kbp = 32M
% cache/readCounts/1000kbp = 5.2M

\clearpage

\section{Generating bin annotations}

<<eval=FALSE>>=
# load required packages for human reference genome build hg19
library(QDNAseq)
library(Biobase)
library(BSgenome.Hsapiens.UCSC.hg19)

# set the bin size
binSize <- 15

# create bins from the reference genome
bins <- createBins(bsgenome=BSgenome.Hsapiens.UCSC.hg19, binSize=binSize)

# calculate mappabilites per bin from ENCODE mapability tracks
bins$mappability <- calculateMappability(bins,
  bigWigFile='/path/to/wgEncodeCrgMapabilityAlign50mer.bigWig',
  bigWigAverageOverBed='/path/to/bigWigAverageOverBed')

# calculate overlap with ENCODE blacklisted regions
bins$blacklist <- calculateBlacklist(bins,
  bedFiles=c('/path/to/wgEncodeDacMapabilityConsensusExcludable.bed',
  '/path/to/wgEncodeDukeMapabilityRegionsExcludable.bed'))

# load data for the 1000 Genomes (or similar) data set, and generate residuals
tg <- binReadCounts(bins,
  path='/path/to/1000Genomes/bam/files')
tg <- applyFilters(tg, residual=FALSE, blacklist=FALSE,
  mappability=FALSE, bases=FALSE)
bins$residual <- iterateResiduals(tg)

# by default, use all autosomal bins that have a reference sequence
# (i.e. not only N's)
bins$use <- bins$chromosome %in% as.character(1:22) & bins$bases > 0

# convert to AnnotatedDataFrame and add metadata
bins <- AnnotatedDataFrame(bins,
  varMetadata=data.frame(labelDescription=c(
  'Chromosome name',
  'Base pair start position',
  'Base pair end position',
  'Percentage of non-N nucleotides (of full bin size)',
  'Percentage of C and G nucleotides (of non-N nucleotides)',
  'Average mappability of 50mers with a maximum of 2 mismatches',
  'Percent overlap with ENCODE blacklisted regions',
  'Median loess residual from 1000 Genomes (50mers)',
  'Whether the bin should be used in subsequent analysis steps'),
  row.names=colnames(bins)))

# add more metadata
attr(bins, 'QDNAseqVersion') <- packageVersion('QDNAseq')
attr(bins, 'QDNAseqDownloaded') <- TRUE
saveRDS(bins, paste('QDNAseq.hg19.', binsize, 'kbp.SR50.rds', sep=''),
  compress='xz')
@

% where to download ENCODE
% how 1000 genomes were downloaded

\section{Session Information}

The version number of R and packages loaded for generating the vignette were:

<<echo=FALSE>>=
sessionInfo()
@

\end{document}

% EOF
